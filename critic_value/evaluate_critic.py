#!/usr/bin/env python3
"""Run the TD3 critic on a CSV of [state + action] samples.

Reads `data/dataset.csv` generated by `generate_dataset.py`, normalizes observations
with the stored running mean/variance, feeds tanh actions to the critic, and
emits `data/value.csv` containing Q1/Q2 estimates.
"""

from __future__ import annotations

import argparse
import pathlib
import sys
from typing import Iterable

import numpy as np
import pandas as pd
import torch

if __package__ in (None, ""):
    _project_root = pathlib.Path(__file__).resolve().parent.parent
    if str(_project_root) not in sys.path:
        sys.path.insert(0, str(_project_root))

from critic_value.common import (  # type: ignore  # noqa: E402
    critic_input_columns,
    default_device,
    derive_dims,
    ensure_columns,
    load_td3_checkpoint,
)
from simulator_humid.agents.rl.training_td3 import TwinQNetwork  # type: ignore  # noqa: E402
from simulator_humid.utils.paths import RL_OUTPUT_DIR  # type: ignore  # noqa: E402


def validate_columns(df_cols: Iterable[str], obs_cols: list[str], action_cols: list[str]) -> None:
    missing = ensure_columns(df_cols, obs_cols + action_cols)
    if missing:
        joined = ", ".join(missing)
        raise ValueError(f"Dataset is missing required columns: {joined}")


def main() -> None:
    parser = argparse.ArgumentParser(description="Evaluate TD3 critic values for a dataset of samples.")
    parser.add_argument(
        "--checkpoint",
        type=pathlib.Path,
        default=RL_OUTPUT_DIR / "td3_policy_final.pt",
        help="Path to TD3 checkpoint (default: outputs/rl/td3_policy_final.pt)",
    )
    parser.add_argument(
        "--dataset",
        type=pathlib.Path,
        default=pathlib.Path(__file__).resolve().parent / "data" / "dataset.csv",
        help="Input CSV containing observation + action columns (default: critic_value/data/dataset.csv)",
    )
    parser.add_argument(
        "--output",
        type=pathlib.Path,
        default=pathlib.Path(__file__).resolve().parent / "data" / "value.csv",
        help="Output CSV for critic scores (default: critic_value/data/value.csv)",
    )
    parser.add_argument("--device", type=str, default=None, help="Torch device (default: auto-detect)")
    args = parser.parse_args()

    device = default_device(args.device)

    config, normalizer, checkpoint = load_td3_checkpoint(args.checkpoint, device=device)
    obs_dim, action_dim = derive_dims(config)
    obs_cols, action_cols = critic_input_columns(config)

    df = pd.read_csv(args.dataset)
    validate_columns(df.columns, obs_cols, action_cols)

    obs_np = df[obs_cols].to_numpy(dtype=np.float32)
    action_np = df[action_cols].to_numpy(dtype=np.float32)
    if obs_np.shape[1] != obs_dim or action_np.shape[1] != action_dim:
        raise RuntimeError(
            f"Shape mismatch: obs {obs_np.shape} expected (*,{obs_dim}), "
            f"actions {action_np.shape} expected (*,{action_dim})"
        )

    critic_hidden = tuple(config.critic_hidden_sizes) if config.critic_hidden_sizes else tuple(config.hidden_sizes)
    critic = TwinQNetwork(obs_dim, action_dim, critic_hidden).to(device)
    if "critic_state_dict" not in checkpoint:
        raise ValueError("Checkpoint missing 'critic_state_dict'.")
    critic.load_state_dict(checkpoint["critic_state_dict"])
    critic.eval()

    with torch.no_grad():
        obs_tensor = torch.from_numpy(obs_np).to(device)
        action_tensor = torch.from_numpy(action_np).to(device)
        obs_norm = normalizer.normalize_tensor(obs_tensor)
        q1, q2 = critic(obs_norm, action_tensor)

    q1_np = q1.squeeze(1).cpu().numpy()
    q2_np = q2.squeeze(1).cpu().numpy()
    q_min = np.minimum(q1_np, q2_np)
    q_mean = 0.5 * (q1_np + q2_np)

    metadata_cols = [c for c in df.columns if c not in set(obs_cols + action_cols)]
    result = pd.DataFrame({
        "sample_id": df["sample_id"] if "sample_id" in df.columns else np.arange(len(df)),
        "q1": q1_np,
        "q2": q2_np,
        "q_min": q_min,
        "q_mean": q_mean,
    })

    for col in metadata_cols:
        if col == "sample_id":
            continue
        result.insert(1, col, df[col])

    args.output.parent.mkdir(parents=True, exist_ok=True)
    result.to_csv(args.output, index=False)
    print(f"Wrote critic values for {len(result)} samples to {args.output}")


if __name__ == "__main__":
    main()
