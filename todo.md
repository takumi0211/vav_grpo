ここには大きく2つのプロジェクトがあります。
1. gpt-oss 20Bというhugging faceのモデルをGRPOで学習するもの（train_grpo.pyなどcritic_value/以外全て。）
   1. このコードはもうすでに学習完了までうまくいっています。（TES control）
   2. なので、このコードをベースとして同じLLMモデル,同じ流れで、他の問題設定に適応させるのが今回です。
2. critic_value/はvavを制御するという別の問題設定が入っていて、1. と同様の方法でLLMを学習させたいと思っています。
   1. ここにはcritic_value/data/dataset.csvにprompt+状態ベクトルがあり、その状態ベクトルとTD3で事前に作成させたクリティックモデル（critic_value/td3_policy_final.pt）でTD3のクリティックをGRPOの報酬器（ただそのあとsoftmaxはする）として活用したいと考えています。
   2. 流れとしては、既存のものと一緒ですが、報酬の計算が変わるイメージです。（前は動的計画法で事前に報酬を決めていましたが、今回はLLMの出力に対してその場で、（「状態ベクトル」+「行動ベクトル」→「クリティックモデル」→報酬）報酬を計算するバージョンに変わりました。
   3. critic_value/README.mdにTD3で事前に作成させたクリティックモデル（critic_value/td3_policy_final.pt）をどうやって活用するか、すなわち報酬モデルをどう作成するかが書いてあります。

つまり、1. でやっていたことを2. の問題設定でやろうというのがこのディレクトリのミッションです。
- 基本的に報酬計算のところ以外は全く同じでお願いします。
  - 例えば、harmony形式に変える、train_grpo.pyのハイパラは同じ（ただ、promptの最大トークン長は異なります）
- また、critic_valueディレクトリは最終的に消したいので、必要なことのみを抜き出して、preprocessing/やtrain_support/に書いてください。

実装をお願いします。何か質問があれば以下に記述してください
- 